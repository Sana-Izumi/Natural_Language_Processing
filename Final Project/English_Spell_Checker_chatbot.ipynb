{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNMSuCdu2dRJlVvNKlGnBh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sana-Izumi/Natural_Language_Processing/blob/main/Final%20Project/English_Spell_Checker_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**English Spell Checker chatbot**\n",
        "\n",
        "---\n",
        "\n",
        "The input word is correctly spelled, chatbot will return \"Nothing Wrong!\". The input word is misspelled, chatbot will suggest the similar word which seems original."
      ],
      "metadata": {
        "id": "0pdcKTeoQVos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(error_rate = 0.1, epochs = 10, batch_size = 64)"
      ],
      "metadata": {
        "id": "4tAQI2fHUvfY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GQTkvCC5iQFy",
        "outputId": "a2b3e88c-a2fc-41e8-c5a6-82f8c1de3d47"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "5919/5919 [==============================] - 529s 89ms/step - loss: 0.6924 - accuracy: 0.5068 - val_loss: 0.6897 - val_accuracy: 0.5101\n",
            "Epoch 2/10\n",
            "5919/5919 [==============================] - 527s 89ms/step - loss: 0.6876 - accuracy: 0.5116 - val_loss: 0.6864 - val_accuracy: 0.5136\n",
            "Epoch 3/10\n",
            "5919/5919 [==============================] - 529s 89ms/step - loss: 0.6861 - accuracy: 0.5139 - val_loss: 0.6857 - val_accuracy: 0.5155\n",
            "Epoch 4/10\n",
            "5919/5919 [==============================] - 532s 90ms/step - loss: 0.6856 - accuracy: 0.5145 - val_loss: 0.6853 - val_accuracy: 0.5157\n",
            "Epoch 5/10\n",
            "5919/5919 [==============================] - 528s 89ms/step - loss: 0.6850 - accuracy: 0.5157 - val_loss: 0.6850 - val_accuracy: 0.5161\n",
            "Epoch 6/10\n",
            "5919/5919 [==============================] - 529s 89ms/step - loss: 0.6846 - accuracy: 0.5165 - val_loss: 0.6848 - val_accuracy: 0.5161\n",
            "Epoch 7/10\n",
            "5919/5919 [==============================] - 527s 89ms/step - loss: 0.6842 - accuracy: 0.5169 - val_loss: 0.6847 - val_accuracy: 0.5163\n",
            "Epoch 8/10\n",
            "5919/5919 [==============================] - 526s 89ms/step - loss: 0.6838 - accuracy: 0.5181 - val_loss: 0.6846 - val_accuracy: 0.5170\n",
            "Epoch 9/10\n",
            "5919/5919 [==============================] - 522s 88ms/step - loss: 0.6835 - accuracy: 0.5189 - val_loss: 0.6847 - val_accuracy: 0.5170\n",
            "Epoch 10/10\n",
            "5919/5919 [==============================] - 567s 96ms/step - loss: 0.6832 - accuracy: 0.5195 - val_loss: 0.6846 - val_accuracy: 0.5153\n",
            "2960/2960 [==============================] - 45s 15ms/step - loss: 0.6846 - accuracy: 0.5153\n",
            "Accuracy: 0.5152648091316223\n",
            "2960/2960 [==============================] - 46s 15ms/step\n",
            "Accuracy: 0.5152647975077882\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.76      0.61     47305\n",
            "           1       0.53      0.27      0.36     47390\n",
            "\n",
            "    accuracy                           0.52     94695\n",
            "   macro avg       0.52      0.52      0.48     94695\n",
            "weighted avg       0.52      0.52      0.48     94695\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "from nltk.corpus import words\n",
        "import random\n",
        "\n",
        "# Download the dataset if not already done\n",
        "nltk.download('words')\n",
        "\n",
        "# Load the dataset\n",
        "word_list = words.words()\n",
        "\n",
        "# Function to introduce misspellings\n",
        "def introduce_misspellings(word, error_rate=0.1):\n",
        "    if random.random() < error_rate:\n",
        "        index = random.randint(0, len(word) - 1)\n",
        "        return word[:index] + random.choice('abcdefghijklmnopqrstuvwxyz') + word[index + 1:]\n",
        "    return word\n",
        "\n",
        "# Create a dataset with misspellings\n",
        "misspelled_word_list = [introduce_misspellings(word) for word in word_list]\n",
        "\n",
        "# Combine correct and misspelled words\n",
        "combined_word_list = word_list + misspelled_word_list\n",
        "labels = [0] * len(word_list) + [1] * len(misspelled_word_list)\n",
        "\n",
        "# Tokenize the words\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(combined_word_list)\n",
        "X_seq = tokenizer.texts_to_sequences(combined_word_list)\n",
        "max_seq_length = max(len(seq) for seq in X_seq)\n",
        "X_seq = pad_sequences(X_seq, maxlen=max_seq_length)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_seq, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=50, input_length=max_seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "filename = 'chatbot_errorRate10_epochs10_batch64.h5'\n",
        "model.save(filename)"
      ],
      "metadata": {
        "id": "pNnRY41qRVRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load the libraries and dataset"
      ],
      "metadata": {
        "id": "0KPMLbxURJmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "from nltk.corpus import words\n",
        "import random\n",
        "\n",
        "# Download the dataset if not already done\n",
        "nltk.download('words')\n",
        "\n",
        "# Load the dataset\n",
        "word_list = words.words()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeuR1XJX-Slu",
        "outputId": "866ea057-b40f-470d-dcba-26f647595e6d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create a Corpus with Misspelled Words\n",
        "\n",
        "Making the dataset mixed words correctly spelling and misspelling"
      ],
      "metadata": {
        "id": "DNHpMVojRygL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to introduce misspellings\n",
        "def introduce_misspellings(word, error_rate=0.8):\n",
        "    if random.random() < error_rate:\n",
        "        index = random.randint(0, len(word) - 1)\n",
        "        return word[:index] + random.choice('abcdefghijklmnopqrstuvwxyz') + word[index + 1:]\n",
        "    return word\n",
        "\n",
        "# Create a dataset with misspellings\n",
        "misspelled_word_list = [introduce_misspellings(word) for word in word_list]\n",
        "\n",
        "# Combine correct and misspelled words\n",
        "combined_word_list = word_list + misspelled_word_list\n",
        "labels = [0] * len(word_list) + [1] * len(misspelled_word_list)\n"
      ],
      "metadata": {
        "id": "GYI6E31Y-QXq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 3. Implementing LSTM-based neural network using Keras\n",
        "\n",
        "1. Preprocessing Data: Tokenize the words and convert them into sequences.\n",
        "2. Padding Sequences: Neural networks require input sequences to be of the same length.\n",
        "3. Building the LSTM Model: Build a sequential model with an embedding layer, LSTM layers, and a dense output layer.\n",
        "4. Training the Model: Fit the model on the training data.\n",
        "5. Evaluating the Model: Evaluate the model's performance on the test data.\n"
      ],
      "metadata": {
        "id": "6vozuefpS5yH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the words\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(combined_word_list)\n",
        "X_seq = tokenizer.texts_to_sequences(combined_word_list)\n",
        "max_seq_length = max(len(seq) for seq in X_seq)\n",
        "X_seq = pad_sequences(X_seq, maxlen=max_seq_length)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_seq, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=50, input_length=max_seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgMwJska-b8V",
        "outputId": "411e20f7-539c-4f23-c5fc-3f93abf896de"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "5919/5919 [==============================] - 601s 101ms/step - loss: 0.6033 - accuracy: 0.6455 - val_loss: 0.5794 - val_accuracy: 0.6721\n",
            "Epoch 2/10\n",
            "5919/5919 [==============================] - 592s 100ms/step - loss: 0.5610 - accuracy: 0.6909 - val_loss: 0.5489 - val_accuracy: 0.7020\n",
            "Epoch 3/10\n",
            "5919/5919 [==============================] - 553s 93ms/step - loss: 0.5378 - accuracy: 0.7131 - val_loss: 0.5321 - val_accuracy: 0.7181\n",
            "Epoch 4/10\n",
            "5919/5919 [==============================] - 602s 102ms/step - loss: 0.5188 - accuracy: 0.7296 - val_loss: 0.5196 - val_accuracy: 0.7285\n",
            "Epoch 5/10\n",
            "5919/5919 [==============================] - 586s 99ms/step - loss: 0.5042 - accuracy: 0.7415 - val_loss: 0.5122 - val_accuracy: 0.7360\n",
            "Epoch 6/10\n",
            "5919/5919 [==============================] - 584s 99ms/step - loss: 0.4927 - accuracy: 0.7497 - val_loss: 0.5070 - val_accuracy: 0.7400\n",
            "Epoch 7/10\n",
            "5919/5919 [==============================] - 588s 99ms/step - loss: 0.4822 - accuracy: 0.7576 - val_loss: 0.5049 - val_accuracy: 0.7458\n",
            "Epoch 8/10\n",
            "5919/5919 [==============================] - 583s 99ms/step - loss: 0.4736 - accuracy: 0.7640 - val_loss: 0.5108 - val_accuracy: 0.7409\n",
            "Epoch 9/10\n",
            "5919/5919 [==============================] - 589s 99ms/step - loss: 0.4659 - accuracy: 0.7692 - val_loss: 0.5040 - val_accuracy: 0.7460\n",
            "Epoch 10/10\n",
            "5919/5919 [==============================] - 593s 100ms/step - loss: 0.4583 - accuracy: 0.7746 - val_loss: 0.5098 - val_accuracy: 0.7451\n",
            "2960/2960 [==============================] - 49s 16ms/step - loss: 0.5098 - accuracy: 0.7451\n",
            "Accuracy: 0.7451396584510803\n",
            "2960/2960 [==============================] - 48s 16ms/step\n",
            "Accuracy: 0.7451396589049052\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.89      0.78     47305\n",
            "           1       0.84      0.60      0.70     47390\n",
            "\n",
            "    accuracy                           0.75     94695\n",
            "   macro avg       0.77      0.75      0.74     94695\n",
            "weighted avg       0.77      0.75      0.74     94695\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "filename = 'chatbot_errorRate80_epochs10_batch64.h5'\n",
        "model.save(filename)"
      ],
      "metadata": {
        "id": "4Ioz_lk3TmSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-rXHS6EQTw5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(error_rate = 0.5, epochs = 10, batch_size = 32)"
      ],
      "metadata": {
        "id": "FmLIuXOiToy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the words\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(combined_word_list)\n",
        "X_seq = tokenizer.texts_to_sequences(combined_word_list)\n",
        "max_seq_length = max(len(seq) for seq in X_seq)\n",
        "X_seq = pad_sequences(X_seq, maxlen=max_seq_length)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_seq, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=50, input_length=max_seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8DZH5Le_kbt",
        "outputId": "8cf51425-4abc-4eee-d710-6c67fe308101"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "11837/11837 [==============================] - 617s 52ms/step - loss: 0.6477 - accuracy: 0.5860 - val_loss: 0.6319 - val_accuracy: 0.6070\n",
            "Epoch 2/10\n",
            "11837/11837 [==============================] - 608s 51ms/step - loss: 0.6259 - accuracy: 0.6140 - val_loss: 0.6206 - val_accuracy: 0.6193\n",
            "Epoch 3/10\n",
            "11837/11837 [==============================] - 566s 48ms/step - loss: 0.6158 - accuracy: 0.6253 - val_loss: 0.6156 - val_accuracy: 0.6259\n",
            "Epoch 4/10\n",
            "11837/11837 [==============================] - 607s 51ms/step - loss: 0.6075 - accuracy: 0.6348 - val_loss: 0.6096 - val_accuracy: 0.6337\n",
            "Epoch 5/10\n",
            "11837/11837 [==============================] - 605s 51ms/step - loss: 0.6008 - accuracy: 0.6414 - val_loss: 0.6061 - val_accuracy: 0.6359\n",
            "Epoch 6/10\n",
            "11837/11837 [==============================] - 563s 48ms/step - loss: 0.5954 - accuracy: 0.6462 - val_loss: 0.6044 - val_accuracy: 0.6391\n",
            "Epoch 7/10\n",
            "11837/11837 [==============================] - 604s 51ms/step - loss: 0.5906 - accuracy: 0.6504 - val_loss: 0.6142 - val_accuracy: 0.6365\n",
            "Epoch 8/10\n",
            "11837/11837 [==============================] - 606s 51ms/step - loss: 0.5869 - accuracy: 0.6538 - val_loss: 0.6048 - val_accuracy: 0.6405\n",
            "Epoch 9/10\n",
            "11837/11837 [==============================] - 572s 48ms/step - loss: 0.5833 - accuracy: 0.6564 - val_loss: 0.6039 - val_accuracy: 0.6399\n",
            "Epoch 10/10\n",
            "11837/11837 [==============================] - 615s 52ms/step - loss: 0.5802 - accuracy: 0.6592 - val_loss: 0.6047 - val_accuracy: 0.6408\n",
            "2960/2960 [==============================] - 48s 16ms/step - loss: 0.6047 - accuracy: 0.6408\n",
            "Accuracy: 0.6407941579818726\n",
            "2960/2960 [==============================] - 47s 16ms/step\n",
            "Accuracy: 0.6407941285178732\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.91      0.72     47305\n",
            "           1       0.81      0.37      0.51     47390\n",
            "\n",
            "    accuracy                           0.64     94695\n",
            "   macro avg       0.70      0.64      0.61     94695\n",
            "weighted avg       0.70      0.64      0.61     94695\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "filename = 'chatbot_errorRate50_epochs10_batch32.h5'\n",
        "model.save(filename)"
      ],
      "metadata": {
        "id": "R-5wEdyZVy0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_q0NxQyoUdF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chatbot function to correct spelling**\n",
        "\n",
        "Enter a word. Type 'exit' to quit."
      ],
      "metadata": {
        "id": "60NQdAZfT-Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chatbot function to correct spelling\n",
        "def correct_spelling(input_word):\n",
        "    input_seq = tokenizer.texts_to_sequences([input_word])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_seq_length)\n",
        "\n",
        "    prediction = model.predict(input_seq)\n",
        "\n",
        "    if prediction < 0.5:\n",
        "        return \"Nothing Wrong!\"\n",
        "\n",
        "    # Find the most similar word\n",
        "    similarities = []\n",
        "    for word in word_list:\n",
        "        if len(word) == len(input_word):\n",
        "            sim = sum(1 for a, b in zip(word, input_word) if a == b)\n",
        "            similarities.append((word, sim))\n",
        "    if not similarities:\n",
        "        return \"No suggestions available.\"\n",
        "\n",
        "    best_match = max(similarities, key=lambda x: x[1])[0]\n",
        "    accuracy = max(similarities, key=lambda x: x[1])[1] / len(input_word) * 100\n",
        "\n",
        "    return f\"Suggested Correction: {best_match}, Matching Accuracy: {accuracy:.2f}%\"\n",
        "\n",
        "# Simple chatbot interface\n",
        "def chatbot():\n",
        "    print(\"Welcome to the English Correcting Chatbot! Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        user_input = input(\"Enter a word: \").strip()\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "        correction = correct_spelling(user_input)\n",
        "        print(correction)\n",
        "\n",
        "# Run the chatbot\n",
        "chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxwkssMv601U",
        "outputId": "bf29404d-6165-48d7-e825-05573cb1ed9f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the English Correcting Chatbot! Type 'exit' to quit.\n",
            "Enter a word: hallo\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "Nothing Wrong!\n",
            "Enter a word: heplo\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Nothing Wrong!\n",
            "Enter a word: herlo\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Nothing Wrong!\n",
            "Enter a word: pythrn\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Suggested Correction: python, Matching Accuracy: 83.33%\n",
            "Enter a word: wordd\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Nothing Wrong!\n",
            "Enter a word: wolrd\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "Nothing Wrong!\n",
            "Enter a word: vread\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Suggested Correction: aread, Matching Accuracy: 80.00%\n",
            "Enter a word: ahind\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Suggested Correction: ahind, Matching Accuracy: 100.00%\n",
            "Enter a word: eixt\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Suggested Correction: aint, Matching Accuracy: 50.00%\n",
            "Enter a word: nomm\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Suggested Correction: noma, Matching Accuracy: 75.00%\n",
            "Enter a word: jupan\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Nothing Wrong!\n",
            "Enter a word: wrod\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Suggested Correction: brod, Matching Accuracy: 75.00%\n",
            "Enter a word: vidoo\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Suggested Correction: video, Matching Accuracy: 80.00%\n",
            "Enter a word: exit\n"
          ]
        }
      ]
    }
  ]
}